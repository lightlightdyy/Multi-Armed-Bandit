“”“
https://zhuanlan.zhihu.com/p/32356077

让我们忘记探索阶段和利用阶段，仔细想想如何充分利用历史信息，找到最值得被推荐的菜：

观测 1: 如果一道菜已经推荐了k遍（获取了k次反馈），我们就可以算出菜做的好吃的概率：

\tilde{p} = \frac{\sum{reward_i}}{k}

当k趋近正无穷时，\tilde{p} 会趋近于真实的菜做的好吃的概率 p

观测 2: 现实当中一道菜被试吃的次数k不可能无穷大，因此估计出的好吃的概率\tilde{p}
和真实的好吃的概率 p 总会存在一个差值 \Delta ，即 \tilde{p} - \Delta \le p \le \tilde{p} + \Delta

基于上面两个观测，我们可以定义一个新的策略：每次推荐时，总是乐观地认为每道菜能够获得的回报是 \tilde{p} + \Delta ，
这便是著名的Upper Confidence Bound (UCB) 算法， \Delta = \sqrt{2 \ln {T} / n} 是一个不错的选择。
”“”

import numpy as np

T = 1000 # T个客人
N = 10 # N道菜

true_rewards = np.random.uniform(low=0, high=1, size=N) # 每道菜好吃的概率
estimated_rewards = np.zeros(N) # 每道菜好吃的估计概率
chosen_count = np.zeros(N) #各个菜被选中的次数
total_reward = 0 

def calculate_delta(T, item):
    if chosen_count[item] == 0:
        return 1
    else:
        return np.sqrt(2 * np.log(T) / chosen_count[item])

def UCB(t, N):
    upper_bound_probs = [estimated_rewards[item] + calculate_delta(t, item) for item in range(N)]
    item = np.argmax(upper_bound_probs)
    reward = np.random.binomial(n=1, p=true_rewards[item])
    return item, reward

for t in range(1, T): # T个客人依次进入餐馆
   # 从N道菜中推荐一个，reward = 1 表示客人接受，reward = 0 表示客人拒绝并离开
   item, reward = UCB(t, N)
   total_reward += reward # 一共有多少客人接受了推荐
   
   # 更新菜的平均成功概率
   estimated_rewards[item] = ((t - 1) * estimated_rewards[item] + reward) / t
   chosen_count[item] += 1
